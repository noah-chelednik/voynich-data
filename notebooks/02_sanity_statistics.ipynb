{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Statistics\n",
    "\n",
    "**Purpose:** Compute basic statistics and sanity checks on the EVA lines dataset.\n",
    "\n",
    "This notebook validates data quality and provides an overview of the corpus.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- EVA lines dataset built (`python -m builders.build_eva_lines`)\n",
    "- Dataset available in `output/eva_lines.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is in path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "data_file = project_root / \"output\" / \"eva_lines.parquet\"\n",
    "if not data_file.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_file}. Run build_eva_lines first.\")\n",
    "\n",
    "ds = load_dataset(\"parquet\", data_files=str(data_file), split=\"train\")\n",
    "print(f\"Loaded dataset with {len(ds)} records\")\n",
    "print(f\"\\nColumns: {ds.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EVA Corpus Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic counts\n",
    "total_lines = len(ds)\n",
    "unique_pages = len(set(ds['page_id']))\n",
    "total_chars = sum(ds['char_count'])\n",
    "total_words = sum(ds['word_count'])\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVA CORPUS STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total pages: {unique_pages}\")\n",
    "print(f\"Total lines: {total_lines}\")\n",
    "print(f\"Total characters: {total_chars:,}\")\n",
    "print(f\"Total words: {total_words:,}\")\n",
    "print(f\"Average lines/page: {total_lines / unique_pages:.1f}\")\n",
    "print(f\"Average chars/line: {total_chars / total_lines:.1f}\")\n",
    "print(f\"Average words/line: {total_words / total_lines:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines per page distribution\n",
    "from collections import Counter\n",
    "\n",
    "page_line_counts = Counter(ds['page_id'])\n",
    "line_counts = list(page_line_counts.values())\n",
    "\n",
    "print(\"\\nLines per page distribution:\")\n",
    "print(f\"  Min: {min(line_counts)}\")\n",
    "print(f\"  Max: {max(line_counts)}\")\n",
    "print(f\"  Mean: {sum(line_counts) / len(line_counts):.1f}\")\n",
    "print(f\"  Median: {sorted(line_counts)[len(line_counts)//2]}\")\n",
    "\n",
    "# Show pages with most/fewest lines\n",
    "most_lines = page_line_counts.most_common(5)\n",
    "print(\"\\nPages with most lines:\")\n",
    "for page, count in most_lines:\n",
    "    print(f\"  {page}: {count} lines\")\n",
    "\n",
    "fewest_lines = page_line_counts.most_common()[-5:]\n",
    "print(\"\\nPages with fewest lines:\")\n",
    "for page, count in reversed(fewest_lines):\n",
    "    print(f\"  {page}: {count} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Character Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count all characters (excluding spaces and punctuation)\n",
    "char_counter = Counter()\n",
    "for text in ds['text']:\n",
    "    # Remove spaces and word separators\n",
    "    clean = text.replace(' ', '').replace('.', '').replace(',', '')\n",
    "    char_counter.update(clean)\n",
    "\n",
    "print(\"Top 20 most frequent characters:\")\n",
    "print(\"-\" * 30)\n",
    "for char, count in char_counter.most_common(20):\n",
    "    pct = count / sum(char_counter.values()) * 100\n",
    "    print(f\"  '{char}': {count:>6} ({pct:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character frequency visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    top_chars = char_counter.most_common(20)\n",
    "    chars = [c[0] for c in top_chars]\n",
    "    counts = [c[1] for c in top_chars]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(chars, counts, color='steelblue')\n",
    "    plt.xlabel('EVA Character')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 20 EVA Character Frequencies')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    output_dir = project_root / \"reports\" / \"figures\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(output_dir / \"char_frequency.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved to: {output_dir / 'char_frequency.png'}\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all words and compute length distribution\n",
    "word_lengths = []\n",
    "all_words = []\n",
    "\n",
    "for text in ds['text']:\n",
    "    # Split on spaces and period separators\n",
    "    words = text.replace('.', ' ').replace(',', ' ').split()\n",
    "    for word in words:\n",
    "        if word:  # Skip empty strings\n",
    "            word_lengths.append(len(word))\n",
    "            all_words.append(word)\n",
    "\n",
    "length_counter = Counter(word_lengths)\n",
    "\n",
    "print(\"Word length distribution:\")\n",
    "print(\"-\" * 30)\n",
    "for length in sorted(length_counter.keys()):\n",
    "    count = length_counter[length]\n",
    "    pct = count / len(word_lengths) * 100\n",
    "    bar = '█' * int(pct / 2)\n",
    "    print(f\"  {length:>2} chars: {count:>5} ({pct:>5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word length statistics\n",
    "print(\"\\nWord length statistics:\")\n",
    "print(f\"  Total words: {len(word_lengths):,}\")\n",
    "print(f\"  Min length: {min(word_lengths)}\")\n",
    "print(f\"  Max length: {max(word_lengths)}\")\n",
    "print(f\"  Mean length: {sum(word_lengths) / len(word_lengths):.2f}\")\n",
    "print(f\"  Median length: {sorted(word_lengths)[len(word_lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter(all_words)\n",
    "\n",
    "print(\"Top 30 most frequent words:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (word, count) in enumerate(word_counter.most_common(30), 1):\n",
    "    pct = count / len(all_words) * 100\n",
    "    print(f\"  {i:>2}. {word:<15} {count:>5} ({pct:>4.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary statistics\n",
    "unique_words = len(word_counter)\n",
    "hapax_legomena = sum(1 for w, c in word_counter.items() if c == 1)\n",
    "dis_legomena = sum(1 for w, c in word_counter.items() if c == 2)\n",
    "\n",
    "print(\"\\nVocabulary statistics:\")\n",
    "print(f\"  Unique words (types): {unique_words:,}\")\n",
    "print(f\"  Total words (tokens): {len(all_words):,}\")\n",
    "print(f\"  Type-token ratio: {unique_words / len(all_words):.4f}\")\n",
    "print(f\"  Hapax legomena (appear once): {hapax_legomena} ({hapax_legomena/unique_words*100:.1f}%)\")\n",
    "print(f\"  Dis legomena (appear twice): {dis_legomena} ({dis_legomena/unique_words*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Section Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines and pages by section\n",
    "section_stats = {}\n",
    "\n",
    "for record in ds:\n",
    "    section = record['section'] or 'unknown'\n",
    "    if section not in section_stats:\n",
    "        section_stats[section] = {'lines': 0, 'pages': set(), 'chars': 0, 'words': 0}\n",
    "    section_stats[section]['lines'] += 1\n",
    "    section_stats[section]['pages'].add(record['page_id'])\n",
    "    section_stats[section]['chars'] += record['char_count']\n",
    "    section_stats[section]['words'] += record['word_count']\n",
    "\n",
    "print(\"Statistics by manuscript section:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Section':<20} {'Pages':>8} {'Lines':>8} {'Chars':>10} {'Words':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for section in sorted(section_stats.keys()):\n",
    "    stats = section_stats[section]\n",
    "    print(f\"{section:<20} {len(stats['pages']):>8} {stats['lines']:>8} {stats['chars']:>10,} {stats['words']:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sections = sorted(section_stats.keys())\n",
    "    page_counts = [len(section_stats[s]['pages']) for s in sections]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(sections, page_counts, color='forestgreen')\n",
    "    plt.xlabel('Number of Pages')\n",
    "    plt.ylabel('Section')\n",
    "    plt.title('Pages per Manuscript Section')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_dir / \"pages_per_section.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved to: {output_dir / 'pages_per_section.png'}\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Currier Language Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Currier language distribution\n",
    "lang_stats = {}\n",
    "\n",
    "for record in ds:\n",
    "    lang = record['currier_language'] or 'unknown'\n",
    "    if lang not in lang_stats:\n",
    "        lang_stats[lang] = {'lines': 0, 'pages': set()}\n",
    "    lang_stats[lang]['lines'] += 1\n",
    "    lang_stats[lang]['pages'].add(record['page_id'])\n",
    "\n",
    "print(\"Currier language distribution:\")\n",
    "print(\"-\" * 40)\n",
    "for lang in sorted(lang_stats.keys()):\n",
    "    stats = lang_stats[lang]\n",
    "    print(f\"  Language {lang}: {len(stats['pages'])} pages, {stats['lines']} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Line Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line type analysis\n",
    "line_type_counter = Counter(ds['line_type'])\n",
    "\n",
    "print(\"Line type distribution:\")\n",
    "print(\"-\" * 40)\n",
    "for lt, count in line_type_counter.most_common():\n",
    "    pct = count / len(ds) * 100\n",
    "    print(f\"  {lt}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential issues\n",
    "issues = []\n",
    "\n",
    "# Check for empty lines\n",
    "empty_lines = sum(1 for r in ds if not r['text'].strip())\n",
    "if empty_lines > 0:\n",
    "    issues.append(f\"Empty lines: {empty_lines}\")\n",
    "else:\n",
    "    print(\"✓ No empty lines\")\n",
    "\n",
    "# Check for duplicate line IDs\n",
    "line_ids = list(ds['line_id'])\n",
    "duplicate_ids = len(line_ids) - len(set(line_ids))\n",
    "if duplicate_ids > 0:\n",
    "    issues.append(f\"Duplicate line IDs: {duplicate_ids}\")\n",
    "else:\n",
    "    print(\"✓ No duplicate line IDs\")\n",
    "\n",
    "# Check for lines with uncertain readings\n",
    "uncertain_lines = sum(1 for r in ds if r['has_uncertain'])\n",
    "print(f\"ℹ Lines with uncertain readings (?): {uncertain_lines} ({uncertain_lines/len(ds)*100:.1f}%)\")\n",
    "\n",
    "# Check for lines with illegible markers\n",
    "illegible_lines = sum(1 for r in ds if r['has_illegible'])\n",
    "print(f\"ℹ Lines with illegible markers (!): {illegible_lines} ({illegible_lines/len(ds)*100:.1f}%)\")\n",
    "\n",
    "# Check character counts\n",
    "zero_char_lines = sum(1 for r in ds if r['char_count'] == 0)\n",
    "if zero_char_lines > 0:\n",
    "    issues.append(f\"Lines with zero characters: {zero_char_lines}\")\n",
    "else:\n",
    "    print(\"✓ All lines have characters\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "if issues:\n",
    "    print(\"⚠ DATA QUALITY ISSUES FOUND:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"✓ ALL DATA QUALITY CHECKS PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Report Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify build report\n",
    "report_file = project_root / \"output\" / \"eva_lines_build_report.json\"\n",
    "if report_file.exists():\n",
    "    with open(report_file) as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    print(\"Build Report:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Source: {report.get('source', 'unknown')}\")\n",
    "    print(f\"  Source hash: {report.get('source_hash', 'unknown')[:16]}...\")\n",
    "    print(f\"  Build time: {report.get('build_time', 'unknown')}\")\n",
    "    print(f\"  Total pages: {report.get('total_pages', 'unknown')}\")\n",
    "    print(f\"  Total lines: {report.get('total_lines', 'unknown')}\")\n",
    "\n",
    "    # Verify against actual data\n",
    "    print(\"\\nVerification:\")\n",
    "    if report.get('total_lines') == len(ds):\n",
    "        print(\"  ✓ Line count matches\")\n",
    "    else:\n",
    "        print(f\"  ✗ Line count mismatch: report={report.get('total_lines')}, actual={len(ds)}\")\n",
    "\n",
    "    if report.get('total_pages') == unique_pages:\n",
    "        print(\"  ✓ Page count matches\")\n",
    "    else:\n",
    "        print(f\"  ✗ Page count mismatch: report={report.get('total_pages')}, actual={unique_pages}\")\n",
    "else:\n",
    "    print(\"⚠ Build report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary = {\n",
    "    \"corpus_statistics\": {\n",
    "        \"total_pages\": unique_pages,\n",
    "        \"total_lines\": total_lines,\n",
    "        \"total_characters\": total_chars,\n",
    "        \"total_words\": total_words,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"hapax_legomena\": hapax_legomena,\n",
    "        \"type_token_ratio\": unique_words / len(all_words),\n",
    "        \"avg_line_length_chars\": total_chars / total_lines,\n",
    "        \"avg_word_length\": sum(word_lengths) / len(word_lengths),\n",
    "    },\n",
    "    \"section_statistics\": {\n",
    "        section: {\n",
    "            \"pages\": len(stats['pages']),\n",
    "            \"lines\": stats['lines'],\n",
    "            \"chars\": stats['chars'],\n",
    "            \"words\": stats['words'],\n",
    "        }\n",
    "        for section, stats in section_stats.items()\n",
    "    },\n",
    "    \"currier_language\": {\n",
    "        lang: {\n",
    "            \"pages\": len(stats['pages']),\n",
    "            \"lines\": stats['lines'],\n",
    "        }\n",
    "        for lang, stats in lang_stats.items()\n",
    "    },\n",
    "    \"line_types\": dict(line_type_counter),\n",
    "    \"top_20_words\": [{'word': w, 'count': c} for w, c in word_counter.most_common(20)],\n",
    "    \"top_20_characters\": [{'char': c, 'count': n} for c, n in char_counter.most_common(20)],\n",
    "    \"data_quality\": {\n",
    "        \"empty_lines\": empty_lines,\n",
    "        \"duplicate_line_ids\": duplicate_ids,\n",
    "        \"uncertain_lines\": uncertain_lines,\n",
    "        \"illegible_lines\": illegible_lines,\n",
    "        \"issues\": issues,\n",
    "        \"all_checks_passed\": len(issues) == 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = project_root / \"reports\" / \"sanity_statistics.json\"\n",
    "output_file.parent.mkdir(exist_ok=True)\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary statistics saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SANITY STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {len(ds):,} lines across {unique_pages} pages\")\n",
    "print(f\"Vocabulary: {unique_words:,} unique words from {len(all_words):,} total\")\n",
    "print(f\"Quality: {'✓ All checks passed' if len(issues) == 0 else '⚠ Issues found'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
